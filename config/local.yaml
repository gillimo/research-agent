# Local runtime and model defaults
local_model: phi3
ollama_host: http://localhost:11434

# Embeddings + vector store (chosen for lightweight local use)
# Default to public model to avoid auth issues; override if you have HF token.
embedding_model: all-MiniLM-L6-v2
vector_store:
  type: faiss
  index_path: data/index/faiss.index
  mock_index_path: data/index/mock_index.pkl
  warm_on_start: false

# Paths
data_paths:
  raw: data/raw
  processed: data/processed
  index: data/index
  logs: logs

# Cloud bridge (no secrets here; use env vars)
cloud:
  enabled: false
  provider: ""          # e.g., "openai", "gemini"
  model: ""             # e.g., "gpt-4o"
  cmd_template: ""      # e.g., 'codex --model gpt-4o --prompt "{prompt}"'
  trigger_score: 0.3
  trigger_on_disagreement: true
  trigger_on_low_confidence: true
  low_confidence_threshold: 0.25
  trigger_on_empty_or_decline: true
  disagreement_phrases:
    - "no"
    - "nope"
    - "not that"
    - "not what i asked"
    - "wrong"
    - "that's wrong"
    - "not correct"
    - "doesn't help"
    - "try again"
    - "you're wrong"
    - "that is wrong"
    - "think hard"
    - "think harder"
    - "be precise"
    - "be more precise"
    - "give me a better answer"
    - "not good enough"

# Auto-update
auto_update:
  ingest_threshold: 0.1
  ingest_cloud_answers: false
  sources_on_gap: true

# UX
rephraser:
  enabled: false

# Execution controls
execution:
  approval_policy: on-request   # on-request|on-failure|never
  sandbox_mode: workspace-write # read-only|workspace-write|full
  command_allowlist: []
  command_denylist: []

# Local-only mode (blocks all cloud calls)
local_only: false

# Context harvesting
context:
  auto: false
  max_recent: 10

# Direct socket server for bi-directional communication
socket_server:
  host: "127.0.0.1"
  port: 6001
