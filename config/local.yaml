# Local runtime and model defaults
local_model: phi3
ollama_host: http://localhost:11434

# Embeddings + vector store (chosen for lightweight local use)
# Default to public model to avoid auth issues; override if you have HF token.
embedding_model: all-MiniLM-L6-v2
vector_store:
  type: faiss
  index_path: data/index/faiss.index
  mock_index_path: data/index/mock_index.pkl
  warm_on_start: false

# Paths
data_paths:
  raw: data/raw
  processed: data/processed
  index: data/index
  logs: logs

# Cloud bridge (no secrets here; use env vars)
cloud:
  enabled: false         # leave off by default; flip on when you have quota/keys
  provider: "openai"     # e.g., "openai", "gemini"
  model: "gpt-5-mini"    # e.g., "gpt-4o"
  cmd_template: ""       # e.g., 'codex --model gpt-4o --prompt "{prompt}"'
  trigger_score: 0.3
  trigger_on_disagreement: true
  trigger_on_low_confidence: true
  low_confidence_threshold: 0.25
  trigger_on_empty_or_decline: true
  disagreement_phrases:
    - "no"
    - "nope"
    - "not that"
    - "not what i asked"
    - "wrong"
    - "that's wrong"
    - "not correct"
    - "doesn't help"
    - "try again"
    - "you're wrong"
    - "that is wrong"
    - "think hard"
    - "think harder"
    - "be precise"
    - "be more precise"
    - "give me a better answer"
    - "not good enough"

# Auto-update
auto_update:
  ingest_threshold: 0.1
  ingest_cloud_answers: false
  sources_on_gap: true

# UX
rephraser:
  enabled: false

# Behavior
behavior:
  summaries: true
  followup_resolver: true
  clarification_policy: true
  context_block: true

# Logging
logging:
  verbose: true

# Execution controls
execution:
  approval_policy: on-request   # on-request|on-failure|never
  sandbox_mode: workspace-write # read-only|workspace-write|full
  command_allowlist: []
  command_denylist: []
  hard_block_outside: false
  allowed_roots: []
  remote_policy: block # block|relay

# Local-only mode (blocks all cloud calls)
local_only: true

# Context harvesting
context:
  auto: false
  max_recent: 10

# UI
ui:
  footer: true
  api_progress: false
  startup_compact: true

# Local LLM
local_llm:
  enabled: true
  streaming: false
  fallbacks: []

# Ingest policy
ingest:
  allowlist_roots: []
  allowlist_exts: []
  allowlist_mode: warn
  scan_proprietary: true
  scan_mode: warn
  scan_max_bytes: 200000

# Trust policy
trust_policy:
  allow_cloud: true
  allow_librarian_notes: true
  allow_sources: ["internal", "public"]
  default_source: "internal"
  cloud_source: "public"
  encrypt_exports: false
  encrypt_when_remote: true
  encrypt_logs: false
  encrypt_logs_when_remote: true
  encryption_key_env: "MARTIN_ENCRYPTION_KEY"

# Remote transport
remote_transport:
  type: "ssh"
  ssh_user: ""
  ssh_host: ""
  local_port: 6001
  remote_port: 6001
  identity_file: ""

# Direct socket server for bi-directional communication
socket_server:
  host: "127.0.0.1"
  port: 6001
  verbose: false
